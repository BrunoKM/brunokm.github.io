<!doctype html>
<html>
<head>
    <link rel="icon" href="images/tab_icon.png">
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Bruno K.M. - Projects</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:300,400,600,700,800,900" rel="stylesheet">
    <link rel="stylesheet" href="scribbler-global.css">
    <link rel="stylesheet" href="scribbler-projects.css">
    <link rel="author" href="humans.txt">

    <script type="text/javascript" src="./lib/jquery-3.4.1.min.js"></script>
    <script src="json/path.json" id="ensembleProbsJson"></script>

</head>
<body>
<!-- NAVIGATION MENU -->
<div class="top_container">

<div class="logo"><img src="images/logo.png"></div>
    <nav>

        <ul class="menu">
            <!-- <div class="menu__item toggle"><span></span></div> -->
            <li class="menu_item rec"><a href="index.html" class="link link--white">Home</a></li>
            <li class="menu_item rec"><a href="blog.html" class="link link--white">Blog</a></li>
            <li class="menu_item rec"><a href="projects.html" class="link link--active">Projects</a></li>
            <li class="menu_item"><a href="https://github.com/BrunoKM/" class="link link--white"><i
                    class="fa fa-github"></i> Github</a></li>
            <li class="menu_item"><a href="https://twitter.com/BMlodozeniec" class="link link--white"><i
                    class="fa fa-twitter"></i> Twitter</a></li>
        </ul>
    </nav>
</div>


<div class="wrapper">

    <!-- Projects Navigation Side Menu -->
    <aside class="doc__nav">
        <ul>
          <li class="js-btn selected">EnDD</li>
         <!-- <li class="js-btn">Harvard Connectomics</li> -->
          <li class="js-btn">CUAI</li>
            <!-- <li class="js-btn">Keybindings</li>
            <li class="js-btn">Issues</li> -->
        </ul>
    </aside>

    <article class="doc__content">
        <!-- Ensemble Distribution Distillation -->
        <section class="js-section">
            <h3 class="section__title">Ensemble Distribution Distillation</h3>
            <p>Neural Networks (NNs) are cool, but obtaining tractable estimates of uncertainty from neural networks is hard, and remains an insomnia inducing phenomenon for Bayesians around the world. Neural Networks tend to make over-confident predictions and, until recently, have been unable to provide measures of uncertainty in their predictions. As they are increasingly being applied to safety-critical tasks such as medical diagnosis, biometric identification and self-driving cars, estimating uncertainty in model’s predictions is essential, as it enables the safety of an AI system to be improved by acting on the predictions in an informed manner.</p>

            <p>Ensembles of deep-learning models offer one way to tackle the uncertainty problem. Ensembles of models have been empirically shown to yield robust measures of uncertainty, and they often yield improvements in system performance. Ensembles are also capable of distinguishing between different forms of uncertainty - they allow total uncertainty in predictions to be decomposed into <i>knowledge uncertainty</i> and <i>data uncertainty</i>.</p>


            <p>Data uncertainty, or aleatoric uncertainty, is the irreducible uncertainty in predictions that arises due to the complexity, noise and class-overlap in the data. For instance, if the algorithm tasked with classifying animals observes an input image with a silhouette so distant it is practically impossible to discern whether it depicts a cat or a dog, that would be an example of data uncertainty.</p>

            <p>On the other hand, knowledge uncertainty is uncertainty due to a lack of understanding or knowledge on the part of the model regarding the current input for which the model is making a prediction. This form of uncertainty arises when multiple hypotheses consistent with the training data give conflicting prediction on the test data. It usually occurs when the test input is unlike anything the model has seen before.  For instance, knowledge uncertainty might arise if a self-driving algorithm was trained exclusively on video input in daylight conditions, and was applied when driving at night. Mismatch between the test and training distributions, also known as a dataset shift, is a situation that often arises for real world problems. </p>
	<h4>Interactive Deep Ensemble uncertainty decomposition demo</h4>

            <p>To illustrate the ability of neural network (NN) ensembles to capture different kinds of uncertainty, an ensemble of 20 deep NNs has been trained from different random initialisations on a 3-class toy dataset shown below. Each colour in the plot corresponds to one of the 3 classes.</p>
            <div class="projects_img"><img style="width: 44%;" src="images/dataset_spiral.png" id="dataset_spiral"></div>

            <p>The 2D dimensionality of the dataset is well-suited for visualisation. The figure below shows the total, knowledge and data uncertainties (left), and interactively displays the predictions from each of the 20 models from the ensemble as a point on a simplex (right). For example, a point residing in a corner of the simplex implies 100% confidence in that class, whereas a point residing in the middle of the simplex represents the model being 33.3…% confident in each of the three classes.</p>

            <!-- Fancy UCNERTAINTY FIGURE -->
            <div id='uncertainty_selection_buttons' class='plotSelectionButtons'>
              <ul>
                <li class='unc-img-selector-button selected' id='tot_unc_button'>Total Uncertainty</li>
                <li class='unc-img-selector-button' id='data_unc_button'>Data Uncertainty</li>
                <li class='unc-img-selector-button' id='know_unc_button'>Knowledge Uncertainty</li>
              </ul>
            </div>
            <div id="unc_figure" style="
              align-items: left">
                <img src="images/total_uncertainty.png" id="uncertainty_pic" style="border-radius: 4px; width:40%; margin-left: 4%; margin-right: 4%;">
                <canvas id="canvas" style="width:40%"></canvas>
            </div>

            <p>Distinguishing between sources of uncertainty is important, as in many machine learning applications it is often necessary to know not only whether the model is uncertain, but also why. For instance, in active learning scenarios, additional training data should be collected from regions with high knowledge uncertainty, but not data uncertainty. Similarly, for a model deployed in real-world production environments, a rising knowledge uncertainty may signal to the developers that a distributional shift has occurred, and an updated model is due for deployment.</p>

            <p>Ensembles do, however, come at a computational and memory cost which may be prohibitive for many applications. There has been a lot of work done on the distillation of an ensemble into a single model and such approaches decrease computational cost and allow a single model to achieve an accuracy comparable to that of an ensemble. However, in these approaches, information about the diversity of the ensemble, which can yield estimates of different forms of uncertainty, is lost.</p>

            <p>Recently, a new class of models called <i>Prior Networks</i> has been proposed, which allows a single neural network to explicitly model a distribution over output distributions. <a
                    class="link link--dark"
                    href="https://arxiv.org/abs/1905.00076"
                    target="_blank">In this paper</a>, we combined Ensemble Distillation and Prior Networks to yield a novel approach called <i>Ensemble Distribution Distillation</i> (EnDD) in which the distribution of an ensemble is distilled into a single Prior Network. This enables a single model to retain both the improved classification performance as well as measures of diversity of the ensemble.</p>


            <div style="text-align: center; margin-top:30px">
                <a style="text-align: center; margin-right: 10px;" href="https://arxiv.org/abs/1905.00076"
                   class="link link--dark"><i class="fa fa-book"></i> Ensemble Distribution Distillation Paper</a></div>
        </section>
        <section class="js-section">

        <!-- CUAI CUMIN -->
        <section class="js-section">
            <h3 class="section__title">Cambridge University Machine Intelligence Network</h3>
            <p>Cambridge University Artificial Intelligence (CUAI) is a student society dedicated to connecting Cambridge students to the wider ML community - whether that's academia, industry, or each other. We do so through events like talks, reading groups, or projects. We have co-founded the society in 2018, and since then, we have grown to a committee of 11 students, a community 900 mailing list members, and have hosted events with speakers ranging from a lab lead research scientists at Google to a co-founder of a Cambridge AI start-up Prowler.io.</p>

                <div style="text-align: center; margin-top:30px">
                    <a style="text-align: center; margin-right: 10px;" href="http://cumin.org.uk/"
                       class="link link--dark"><i class="fa fa-brain"></i> CUAI Website</a></div>
        </section>
    </article>
</div>

<!-- Footer -->
<footer class="footer"><span style="font-weight: bolder;">Email: </span><span
        style="font-weight: lighter;">bk<span>m</span>lodoz<span>eni</span>e<span>c at gmail<i>.</i>co</span>m</span>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="scribbler.js"></script>
<script type="text/javascript" src="js/unc_figure.js">
</script>
</body>
</html>
