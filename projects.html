<!doctype html>
<html>
<head>
    <link rel="icon" href="images/tab_icon.png">
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Bruno K.M. - Projects</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:300,400,600,700,800,900" rel="stylesheet">
    <link rel="stylesheet" href="scribbler-global.css">
    <link rel="stylesheet" href="scribbler-projects.css">
    <link rel="author" href="humans.txt">

    <script type="text/javascript" src="./lib/jquery-3.4.1.min.js"></script>
    <script src="json/path.json" id="ensembleProbsJson"></script>

</head>
<body>
<!-- NAVIGATION MENU -->
<div class="top_container">

<div class="logo"><img src="images/logo.png"></div>
    <nav>

        <ul class="menu">
            <!-- <div class="menu__item toggle"><span></span></div> -->
            <li class="menu_item rec"><a href="index.html" class="link link--white">Home</a></li>
            <li class="menu_item rec"><a href="blog.html" class="link link--white">Blog</a></li>
            <li class="menu_item rec"><a href="projects.html" class="link link--active">Projects</a></li>
            <li class="menu_item"><a href="https://github.com/BrunoKM/" class="link link--white"><i
                    class="fa fa-github"></i> Github</a></li>
            <li class="menu_item"><a href="https://twitter.com/BMlodozeniec" class="link link--white"><i
                    class="fa fa-twitter"></i> Twitter</a></li>
        </ul>
    </nav>
</div>


<div class="wrapper">

    <!-- Projects Navigation Side Menu -->
    <aside class="doc__nav">
        <ul>
          <li class="js-btn selected">EnDD</li>
          <li class="js-btn">Harvard Connectomics</li>
          <li class="js-btn">CUMIN</li>
            <!-- <li class="js-btn">Keybindings</li>
            <li class="js-btn">Issues</li> -->
        </ul>
    </aside>

    <article class="doc__content">
        <!-- Ensemble Distribution Distillation -->
        <section class="js-section">
            <h3 class="section__title">Ensemble Distribution Distillation</h3>
            <p>Neural Networks (NNs) have emerged as the state of the art approach to a variety of machine learning tasks. Nevertheless, one fundamental limitation that hinders its wider adoption remains, namely, a lack of tractable methods to obtain estimates of uncertainty. Neural Networks tend to make over-confident predictions and, until recently, have been unable to provide measures of uncertainty in their predictions. As they are increasingly being applied to safety-critical tasks such as medical diagnosis, biometric identification and self-driving cars, estimating uncertainty in model’s predictions is essential, as it enables the safety of an AI system to be improved by acting on the predictions in an informed manner.</p>

            <p>Ensembles of deep-learning models offer one way to tackle the uncertainty problem. A leading formalism for talking about inferences under uncertainty has been Bayesian inference, and there exists a range of both Bayesian and non-Bayesian methods to constructing ensembles. Ensembles of models have been empirically shown to yield robust measures of uncertainty, and they often yield improvements in system performance. Crucially, ensembles are also capable of distinguishing between different forms of uncertainty - they allow total uncertainty in predictions to be decomposed into <i>knowledge uncertainty</i> and <i>data uncertainty</i>.</p>


            <p>Data uncertainty, or aleatoric uncertainty, is the irreducible uncertainty in predictions that arises due to the complexity, noise and class-overlap in the data. For instance, if the algorithm tasked with classifying animals observes an input image with a silhouette so distant it is practically impossible to discern whether it depicts a cat or a dog, that would be an example of data uncertainty.</p>

            <p>On the other hand, knowledge uncertainty, also known as epistemic uncertainty, is uncertainty due to a lack of understanding or knowledge on the part of the model regarding the current input for which the model is making a prediction. This form of uncertainty arises when the test input comes from an unlikely region of the training data distribution. For instance, knowledge uncertainty might arise if a self-driving algorithm was trained exclusively on video input in daylight conditions, and was applied when driving at night. Mismatch between the test and training distributions, also known as a dataset shift, is a situation that often arises for real world problems. </p>

            <p>To illustrate the ability of neural network (NN) ensembles to capture different kinds of uncertainty, an ensemble of 20 deep NNs has been trained from different random initialisations on a 3-class toy dataset shown below. Each colour in the plot corresponds to one of the 3 classes.</p>
            <div class="projects_img"><img style="width: 44%;" src="images/dataset_spiral.png" id="dataset_spiral"></div>

            <p>The 2D dimensionality of the dataset is well-suited for visualisation. The figure below shows the total, knowledge and data uncertainties (left), and interactively displays the predictions from each of the 20 models from the ensemble as a point on a simplex (right). For example, a point residing in a corner of the simplex implies 100% confidence in that class, whereas a point residing in the middle of the simplex represents the model being 33.3…% confident in each of the three classes.</p>

            <!-- Fancy UCNERTAINTY FIGURE -->
            <div id='uncertainty_selection_buttons' class='plotSelectionButtons'>
              <ul>
                <li class='unc-img-selector-button selected' id='tot_unc_button'>Total Uncertainty</li>
                <li class='unc-img-selector-button' id='data_unc_button'>Data Uncertainty</li>
                <li class='unc-img-selector-button' id='know_unc_button'>Knowledge Uncertainty</li>
              </ul>
            </div>
            <div id="unc_figure" style="
              align-items: left">
                <img src="images/total_uncertainty.png" id="uncertainty_pic" style="border-radius: 4px; width:40%; margin-left: 4%; margin-right: 4%;">
                <canvas id="canvas" style="width:40%"></canvas>
            </div>

            <p>Distinguishing between sources of uncertainty is important, as in many machine learning applications it is often necessary to know not only whether the model is uncertain, but also why. For instance, in active learning scenarios, additional training data should be collected from regions with high knowledge uncertainty, but not data uncertainty. Similarly, for a model deployed in real-world production environments, a rising knowledge uncertainty may signal to the developers that a distributional shift has occurred, and an updated model is due for deployment.</p>

            <p>Ensembles do, however, come at a computational and memory cost which may be prohibitive for many applications. There has been a lot of work done on the distillation of an ensemble into a single model and such approaches decrease computational cost and allow a single model to achieve an accuracy comparable to that of an ensemble. However, in these approaches, information about the diversity of the ensemble, which can yield estimates of different forms of uncertainty, is lost.</p>

            <p>Recently, a new class of models called <i>Prior Networks</i> has been proposed, which allows a single neural network to explicitly model a distribution over output distributions. <a
                    class="link link--dark"
                    href="https://arxiv.org/abs/1905.00076"
                    target="_blank">In this paper</a>, we combined Ensemble Distillation and Prior Networks to yield a novel approach called <i>Ensemble Distribution Distillation</i> (EnDD) in which the distribution of an ensemble is distilled into a single Prior Network. This enables a single model to retain both the improved classification performance as well as measures of diversity of the ensemble.</p>


            <div style="text-align: center; margin-top:30px">
                <a style="text-align: center; margin-right: 10px;" href="https://arxiv.org/abs/1905.00076"
                   class="link link--dark"><i class="fa fa-book"></i> Ensemble Distribution Distillation Paper</a></div>
        </section>
        <section class="js-section">

            <!-- HARVARD CONNECTOMICS PROJECT -->
            <h3 class="section__title">Harvard Connectomics Network Analysis Project</h3>
            <p>As an intern at Harvard, I got to work alongside the researchers at the Visual Computing Group (VCG) on
                some of the most prospective research in neuroscience. The group, as part of the collaboration with
                Harvard's Centre for Brain Science, are part of the overarching effort towards completely mapping the
                brain's "connectome" - the complete connectivity diagram of the neuronal circuits within the brain. If
                successful, their work could transform much of neuroscience and could lead to bio-inspired solutions to
                alleviate issues with the current machine learning algorithms <a
                        class="link link--dark"
                        href="https://www.technologyreview.com/s/609070/inside-the-moonshot-effort-to-finally-figure-out-the-brain/"
                        target="_blank">[link]</a>. It is also the essential step towards even attempting to
                reverse-engineer the brain.
            </p>
            <p>The task is highly non-trivial. The cerebral cortex of the human brain contains more than 160 trillion
                synapses, and each neuron receives synaptic inputs from hundreds, or even thousands of different
                neurons, spread out over a large distance. Deciphering the connectome of even one type of neuron in the
                cortex poses enormous challenges.
            </p>

            <div class="projects_img"><img src="images/connectomics.jpg"> A picture of the segmented slice of rodent
                cortex.
            </div>

            <p>For the past few years, the laboratory has been cutting and imaging the brain on nanometre scale with
                multi-beam electron microscopy, and developing systems for 3d reconstruction and segmentation of the
                image into individual neurons and synapses. The next step of the process is to create tools to study
                this data in order to determine what makes the brain process information the way it does.
            </p>

            <p>Throughout my time with the group, I worked on graph analysis tools for connectomics, including
                investigation of the methods for graph matching, graph kernels, graph edit distance estimation with
                graph neural networks, and motif discovery; I was primarily focusing on finding ways to improve upon one
                of the current de-facto methods for biological network analysis: <a class="link link--dark"
                                                                                    href="https://en.wikipedia.org/wiki/Network_motif"
                                                                                    target="_blank">network motif
                    discovery</a>. Network motifs can be thought of as recurrent patterns of connections within a
                network such as that of synapses within the brain. They are particularly interesting within the context
                of connectomics, because brain motifs can often perform certain "computational" functions; for instance,
                the two motifs shown below are examples of a max() operator and a regulator().</p>

            <div class="projects_img" style="margin-bottom: -20px;"><img src="images/brain-motifs.png"></div>

            <p>Brain Networks are particularly challenging for motif discovery analysis. For one, they are extremely
                large; Even just a cube of a rodent cortex with the size 1x1x1 mm already contains roughly 100,000
                neurons, and 100,000,000 synapses. Furthermore, a motif discovery algorithm usually overlooks any
                information other than what other neurons each of the neurons is connected to. In reality, there is a
                wide range of additional parameters that determine the function a neuron performs within the neuronal
                circuit that we would like to take into account; for instance, a neuron can be excitatory or inhibitory,
                or its axons could be myelinated resulting in much stronger signals being transmitted.
            </p>
            <p>To alleviate these issues, we had to look for an exploratory method that finds frequent patterns within
                the network while allowing for a small variation within them, and that can easily take additional
                parameters into account. Allowing for small deviations would make the method significantly more robust
                for larger motifs, and more adaptable to networks with multiple edges and edge parameters such as the
                brain network. One of the key aspects of arriving at such a solution are methods for graph comparison;
                in particular, how to efficiently evaluate whether two patterns are similar.</p>

            <div class="projects_img"><img style="width: 60%;" src="images/motif-discovery-abstract.png"></div>

            <p>Due to recent developments in neural networks that operate on graphs such as <a class="link link--dark"
                                                                                               href="http://snap.stanford.edu/graphsage/"
                                                                                               target="_blank">GraphSAGE</a>
                or <a class="link link--dark"
                      href="https://mila.quebec/en/publication/graph-attention-networks/"
                      target="_blank">Graph Attention Networks</a>, I investigated
                whether these models could be applied for sub-graph or pattern similarity estimation. These methods have
                seen high success in for instance social network analysis scenarios [link], but needed to be heavily
                adapted to this fundamentally different task. The key advantage of using a function approximator such as
                a neural network is that in principle, it should be able to learn any graph similarity (Graph Edit
                Distance) function, while being significantly more computationally efficient. Possibly the largest
                challenge was making sure the neural network inhibits the node permutation invariance features that a
                graph data structure inherently contains.
            </p>


            <div style="text-align: center; margin-top:30px">
                <a style="text-align: center; margin-right: 10px;" href="https://github.com/BrunoKM/rhoana_graph_tools"
                   class="link link--dark"><i class="fa fa-github"></i> Project Code on GitHub</a> | <a
                    style="text-align: center;margin-left: 10px;" href="https://vcg.seas.harvard.edu/projects"
                    class="link link--dark">Link to Group's Website</a></div>

        </section>
        <!-- CUMIN -->
        <section class="js-section">
            <h3 class="section__title">Cambridge University Machine Intelligence Network</h3>
            <p>Cambridge University Machine Intelligence Network (CUMIN) is a student society dedicated to connecting Cambridge students to the wider ML community - whether that's academia, industry, or each other. We do so through events like talks, reading groups, or projects. We have co-founded the society in 2018, and since then, we have grown to a committee of 11 students, a community 900 mailing list members, and have hosted events with speakers ranging from a lab lead research scientists at Google to a co-founder of a Cambridge AI start-up Prowler.io.</p>

                <div style="text-align: center; margin-top:30px">
                    <a style="text-align: center; margin-right: 10px;" href="http://cumin.org.uk/"
                       class="link link--dark"><i class="fa fa-brain"></i> CUMIN Website</a></div>
        </section>
    </article>
</div>

<!-- Footer -->
<footer class="footer"><span style="font-weight: bolder;">Email: </span><span
        style="font-weight: lighter;">bk<span>m</span>lodoz<span>eni</span>e<span>c at gmail<i>.</i>co</span>m</span>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="scribbler.js"></script>
<script type="text/javascript" src="js/unc_figure.js">
</script>
</body>
</html>
