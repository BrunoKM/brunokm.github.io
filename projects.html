<!doctype html>
<html>
<head>
    <link rel="icon" href="images/tab_icon.png">
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Bruno K.M. - Projects</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:300,400,600,700,800,900" rel="stylesheet">
    <link rel="stylesheet" href="scribbler-global.css">
    <link rel="stylesheet" href="scribbler-projects.css">
    <link rel="author" href="humans.txt">

    <script type="text/javascript" src="./lib/jquery-3.4.1.min.js"></script>
    <script src="json/path.json" id="ensembleProbsJson"></script>

</head>
<body>
<!-- NAVIGATION MENU -->
<div class="top_container">
    <div class="logo"><img src="images/logo.png"></div>
    <nav>

        <ul class="menu">
            <!-- <div class="menu__item toggle"><span></span></div> -->
            <li class="menu_item rec"><a href="index.html" class="link link--white">Home</a></li>
            <li class="menu_item rec"><a href="projects.html" class="link link--active"> Projects</a></li>
            <li class="menu_item"><a href="https://github.com/BrunoKM/" class="link link--white"><i
                    class="fa fa-github"></i> Github</a></li>
        </ul>
    </nav>
</div>


<div class="wrapper">

    <!-- Projects Navigation Side Menu -->
    <aside class="doc__nav">
        <ul>
          <li class="js-btn selected">EnDD</li>
          <li class="js-btn">Harvard Connectomics</li>
          <li class="js-btn">CUMIN</li>
            <!-- <li class="js-btn">Keybindings</li>
            <li class="js-btn">Issues</li> -->
        </ul>
    </aside>

    <article class="doc__content">
        <!-- Ensemble Distribution Distillation -->
        <section class="js-section">
            <h3 class="section__title">Ensemble Distribution Distillation</h3>
            <p>Neural Networks (NNs) have emerged as the state of the art approach to a variety of machine learning tasks, however, they tend to make over-confident predictions and, until recently, have been unable to provide measures of uncertainty in their predictions. As they are increasingly being applied to safety-critical tasks such as medical diagnosis, biometric identification and self-driving cars, estimating uncertainty in model’s predictions is essential, as it enables the safety of an AI system to be improved by acting on the predictions in an informed manner.</p>

            <p>Ensembles of models have been empirically shown to yield robust measures of uncertainty, and are capable of distinguishing between different forms of uncertainty. They also often yield improvements in system performance. Ensembles do, however, come at a computational and memory cost which may be prohibitive for many applications. There has been a lot of work done on the distillation of an ensemble into a single model and such approaches decrease computational cost and allow a single model to achieve an accuracy comparable to that of an ensemble. However, in these approaches, information about the diversity of the ensemble, which can yield estimates of different forms of uncertainty, is lost.</p>

            <p>Recently, a new class of models called <i>Prior Networks</i> has been proposed, which allows a single neural network to explicitly model a distribution over output distributions. <a
                    class="link link--dark"
                    href="https://arxiv.org/abs/1905.00076"
                    target="_blank">In this paper</a>, we combined Ensemble Distillation and Prior Networks to yield a novel approach called <i>Ensemble Distribution Distillation</i> (EnDD) in which the distribution of an ensemble is distilled into a single Prior Network. This enables a single model to retain both the improved classification performance as well as measures of diversity of the ensemble.</p>

            <p>Ensembles of models allow total uncertainty in predictions to be decomposed into <i>knowledge uncertainty</i> and <i>data uncertainty</i>, where data uncertainty is the irreducible uncertainty that arises due to the complexity and noise in the data, whereas knowledge uncertainty (epistemic uncertainty) is uncertainty due to a lack of understanding of the model regarding the input. It appears due to limited data and knowledge.</p>

            <p>To illustrate this property of neural network (NN) ensembles, an ensemble of 20 deep NNs has been trained from different random initialisations (with different random order during SGD training) on a 3-class toy dataset shown below. Each colour in the plot corresponds to one of the 3 classes.</p>
            <div class="projects_img"><img style="width: 44%;" src="images/dataset_spiral.png" id="dataset_spiral"></div>

            <p>The 2D dimensionality of the dataset suits itself nicely for visualisation. The figure below shows the total, knowledge and data uncertainties (left), and interactively displays the predictions from each of the 20 models from the ensemble as a point on a simplex (rights). For example, the point residing in a corner of the simplex implies 100% confidence in that class, whereas the point being in the middle of the simplex represents the model being 33.3…% confident in each of the classes.</p>

            <!-- Fancy UCNERTAINTY FIGURE -->
            <div id='uncertainty_selection_buttons' class='plotSelectionButtons'>
              <ul>
                <li class='unc-img-selector-button selected' id='tot_unc_button'>Total Uncertainty</li>
                <li class='unc-img-selector-button' id='data_unc_button'>Data Uncertainty</li>
                <li class='unc-img-selector-button' id='know_unc_button'>Knowledge Uncertainty</li>
              </ul>
            </div>
            <div id="unc_figure" style="
              align-items: left">
                <img src="images/total_uncertainty.png" id="uncertainty_pic" style="border-radius: 4px; width:40%; margin-left: 4%; margin-right: 4%;">
                <canvas id="canvas" style="width:40%"></canvas>
            </div>

            <p>Distiguinshing between sources of uncertainty is important, as in certain machine learning applications it may be necessary to know not only whether the model is uncertain, but also why. For instance, in active learning, additional training data should be collected from regions with high knowledge uncertainty, but not data uncertainty. Similarly, for a regular model, increased knowledge uncertainty may signal to the developers that a distributional shift has occurred and an updated model is due for deployment.</p>
            
            <div style="text-align: center; margin-top:30px">
                <a style="text-align: center; margin-right: 10px;" href="https://arxiv.org/abs/1905.00076"
                   class="link link--dark"><i class="fa fa-book"></i> Ensemble Distribution Distillation Paper</a></div>
        </section>
        <section class="js-section">

            <!-- HARVARD CONNECTOMICS PROJECT -->
            <h3 class="section__title">Harvard Connectomics Network Analysis Project</h3>
            <p>As an intern at Harvard, I got to work alongside the researchers at the Visual Computing Group (VCG) on
                some of the most prospective research in neuroscience. The group, as part of the collaboration with
                Harvard's Centre for Brain Science, are part of the overarching effort towards completely mapping the
                brain's "connectome" - the complete connectivity diagram of the neuronal circuits within the brain. If
                successful, their work could transform much of neuroscience and could lead to bio-inspired solutions to
                alleviate issues with the current machine learning algorithms <a
                        class="link link--dark"
                        href="https://www.technologyreview.com/s/609070/inside-the-moonshot-effort-to-finally-figure-out-the-brain/"
                        target="_blank">[link]</a>. It is also the essential step towards even attempting to
                reverse-engineer the brain.
            </p>
            <p>The task is highly non-trivial. The cerebral cortex of the human brain contains more than 160 trillion
                synapses, and each neuron receives synaptic inputs from hundreds, or even thousands of different
                neurons, spread out over a large distance. Deciphering the connectome of even one type of neuron in the
                cortex poses enormous challenges.
            </p>

            <div class="projects_img"><img src="images/connectomics.jpg"> A picture of the segmented slice of rodent
                cortex.
            </div>

            <p>For the past few years, the laboratory has been cutting and imaging the brain on nanometre scale with
                multi-beam electron microscopy, and developing systems for 3d reconstruction and segmentation of the
                image into individual neurons and synapses. The next step of the process is to create tools to study
                this data in order to determine what makes the brain process information the way it does.
            </p>

            <p>Throughout my time with the group, I worked on graph analysis tools for connectomics, including
                investigation of the methods for graph matching, graph kernels, graph edit distance estimation with
                graph neural networks, and motif discovery; I was primarily focusing on finding ways to improve upon one
                of the current de-facto methods for biological network analysis: <a class="link link--dark"
                                                                                    href="https://en.wikipedia.org/wiki/Network_motif"
                                                                                    target="_blank">network motif
                    discovery</a>. Network motifs can be thought of as recurrent patterns of connections within a
                network such as that of synapses within the brain. They are particularly interesting within the context
                of connectomics, because brain motifs can often perform certain "computational" functions; for instance,
                the two motifs shown below are examples of a max() operator and a regulator().</p>

            <div class="projects_img" style="margin-bottom: -20px;"><img src="images/brain-motifs.png"></div>

            <p>Brain Networks are particularly challenging for motif discovery analysis. For one, they are extremely
                large; Even just a cube of a rodent cortex with the size 1x1x1 mm already contains roughly 100,000
                neurons, and 100,000,000 synapses. Furthermore, a motif discovery algorithm usually overlooks any
                information other than what other neurons each of the neurons is connected to. In reality, there is a
                wide range of additional parameters that determine the function a neuron performs within the neuronal
                circuit that we would like to take into account; for instance, a neuron can be excitatory or inhibitory,
                or its axons could be myelinated resulting in much stronger signals being transmitted.
            </p>
            <p>To alleviate these issues, we had to look for an exploratory method that finds frequent patterns within
                the network while allowing for a small variation within them, and that can easily take additional
                parameters into account. Allowing for small deviations would make the method significantly more robust
                for larger motifs, and more adaptable to networks with multiple edges and edge parameters such as the
                brain network. One of the key aspects of arriving at such a solution are methods for graph comparison;
                in particular, how to efficiently evaluate whether two patterns are similar.</p>

            <div class="projects_img"><img style="width: 60%;" src="images/motif-discovery-abstract.png"></div>

            <p>Due to recent developments in neural networks that operate on graphs such as <a class="link link--dark"
                                                                                               href="http://snap.stanford.edu/graphsage/"
                                                                                               target="_blank">GraphSAGE</a>
                or <a class="link link--dark"
                      href="https://mila.quebec/en/publication/graph-attention-networks/"
                      target="_blank">Graph Attention Networks</a>, I investigated
                whether these models could be applied for sub-graph or pattern similarity estimation. These methods have
                seen high success in for instance social network analysis scenarios [link], but needed to be heavily
                adapted to this fundamentally different task. The key advantage of using a function approximator such as
                a neural network is that in principle, it should be able to learn any graph similarity (Graph Edit
                Distance) function, while being significantly more computationally efficient. Possibly the largest
                challenge was making sure the neural network inhibits the node permutation invariance features that a
                graph data structure inherently contains.
            </p>


            <div style="text-align: center; margin-top:30px">
                <a style="text-align: center; margin-right: 10px;" href="https://github.com/BrunoKM/rhoana_graph_tools"
                   class="link link--dark"><i class="fa fa-github"></i> Project Code on GitHub</a> | <a
                    style="text-align: center;margin-left: 10px;" href="https://vcg.seas.harvard.edu/projects"
                    class="link link--dark">Link to Group's Website</a></div>

        </section>
        <!-- CUMIN -->
        <section class="js-section">
            <h3 class="section__title">Cambridge University Machine Intelligence Network</h3>
            <p>As a team of 4 Cambridge undergraduates, we have founded the CU Machine Intelligence Network. Our aim is
                to become a space for Cambridge students to collaborate with companies and each other on machine
                learning projects,
                provide the tools necessary to do so, run workshops for those who are just getting started, and invite
                interesting speakers from the industry and the academia. As of now, there is no other society that
                allows undergraduates to pursue their machine learning interest collectively, and we are hoping our
                student
                society will contribute to greater activity in that area within the university.</p>
        </section>
    </article>
</div>

<!-- Footer -->
<footer class="footer"><span style="font-weight: bolder;">Email: </span><span
        style="font-weight: lighter;">bk<span>m</span>lodoz<span>eni</span>e<span>c at gmail<i>.</i>co</span>m</span>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="scribbler.js"></script>
<script type="text/javascript" src="js/unc_figure.js">
</script>
</body>
</html>
